{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 109A/STAT 121A/AC 209A/CSCI E-109A: Homework 2\n",
    "# Linear and k-NN Regression\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2017**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader, Rahul Dave, Margo Levine\n",
    "\n",
    "---\n",
    "\n",
    "### INSTRUCTIONS\n",
    "\n",
    "- To submit your assignment follow the instructions given in canvas.\n",
    "- Restart the kernel and run the whole notebook again before you submit. \n",
    "- Do not include your name(s) in the notebook even if you are submitting as a group. \n",
    "- If you submit individually and you have worked with someone, please include the name of your [one] partner below. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.api import OLS\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Taxi Pickups in NYC\n",
    "\n",
    "In this homework, we will explore k-nearest neighbor, linear and polynomial regression methods for predicting a quantitative variable. Specifically, we will build regression models that can predict the number of taxi pickups in New York city at any given time of the day. These prediction models will be useful, for example, in monitoring traffic in the city.\n",
    "\n",
    "The data set for this problem is given in files `dataset_1_train.txt` and `dataset_1_test.txt` as separate training and test sets. The first column in each file contains the time of a day in minutes, and the second column contains the number of pickups observed at that time. The data set covers taxi pickups recorded during different days in Jan 2015 (randomly sampled across days and time of that day).\n",
    "\n",
    "We will fit regression models that use the time of the day (in minutes) as a predictor and predict the average number of taxi pick ups at that time. The models will be fitted to the training set, and  evaluated on the test set. The performance of the models will be evaluated using the $R^2$ metric. \n",
    "\n",
    "**Data Normalization:** As a first step, we suggest that you normalize the `TimeMin` predictor to a value between 0 and 1. This can be done by dividing the time column in the training and test sets by 1440 (i.e. the maximum value the predictor can take). This normalization step would be particularly helpful while fitting polynomial regression models on this data.\n",
    "\n",
    "# Part (0): EDA\n",
    "\n",
    "Generate a scatter plot of the training data points, with the time of the day on the X-axis and the number of taxi pickups on the Y-axis. Does the pattern of taxi pickups make intuitive sense to you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part (a): k-Nearest Neighbors\n",
    "\n",
    "We begin with k-Nearest Neighbors (k-NN), a non-parametric regression technique. You may use `sklearn`'s built-in functions to run k-NN regression. Create a `KNeighborsRegressor` object, use the `fit` method in the object to fit a k-NN regressor model, use the `predict` method to make predictions from the model, and the `score` method to evaluate the $R^2$ score of the model on a data set.\n",
    "\n",
    "- Fit k-NN regression models:\n",
    "    - Fit a k-NN regression model to the training set for different values of $k$ (e.g. you may try out values 1, 2, 10, 25, 50, 100 and 200). \n",
    "    - If you are using `sklearn`'s built-in functions for k-NN regression, explain what happens when you invoke the `fit` function. \n",
    "    - If $n$ is the number of observations in the training set, what can you say about a k-NN regression model that uses $k = n$?\n",
    "- Visualize the fitted models: \n",
    "    - Generate a scatter plot of the training data points, and in the same figure, also generate line plots of the predicted values $\\hat{y}$ from each fitted model as a function of the predictor variable $x$. (*Hint:* you will want to sort the $x$ values before plotting.)\n",
    "    - How does the value of $k$ effect the fitted model?\n",
    "- Evaluate the fitted models:\n",
    "    - Compute the $R^2$ score for the fitted models on both the training and test sets. Are some of the calculated $R^2$ values negative? If so, what does this indicate? What does a $R^2$ score of 0 mean?\n",
    "    - Make plots of the training and test $R^2$ values as a function of $k$. Do the training and test $R^2$ plots exhibit different trends? Explain how the value of $k$ influences the training and test $R^2$ values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part (b): Simple Linear Regression\n",
    "\n",
    "We next consider parametric approaches for regression, starting with simple linear regression, which assumes that the response variable has a linear relationship with the predictor. Do you see any advantages in using a parametric regression model over k-NN regression?\n",
    "\n",
    "We suggest that you use the `statsmodels` module for linear regression. This module has built-in functions to summarize the results of regression, and to compute confidence intervals for estimated regression parameters. Create a `OLS` class instance, use the `fit` method in the instance for fitting a linear regression model, and use the `predict` method to make predictions. To include an intercept term in the regression model, you will need to append a column of 1's to the array of predictors using the `sm.add_constant` method. The `fit` method returns a `results` instance. Use the  `results.summary` method to obtain a summary of the regression fit, the `results.params` attribute to get the estimated regression parameters, and the `conf_int` method to compute confidence intervals for the estimated parameters. You may use the `r2_score` function to compute $R^2$.\n",
    "\n",
    "Using the suggested built-in functions, answer the following questions:\n",
    "\n",
    "- Fit a linear regression model to the training set, and evaluate its $R^2$ value on both the training and test sets (you may notice something peculiar about how they compare). \n",
    "- How does the test $R^2$ score compare with the best test $R^2$ value obtained with k-NN regression in Part (a)?\n",
    "\n",
    "- Compute confidence intervals:\n",
    "    - Print the slope and intercept values for the fitted linear model. What does the sign of the slope convey about the data?\n",
    "    - Compute the 95% confidence interval for the slope and intercept. Based on this information, do you consider the estimates of the model parameters to be reliable?\n",
    "    - Do you expect a 99% confidence interval for the slope and intercept to be tighter or looser than the 95% confidence intervals? Briefly explain your answer.\n",
    "\n",
    "- Analyze residual plots:\n",
    "    - Make a plot of the residuals ${e} = y - \\hat{y}$ of the model on the training set as a function of the predictor variable $x$ (i.e. time of day). Draw a horizontal line denoting the zero residual value on the Y-axis.\n",
    "    - Using this residual plot, comment on whether the assumption of linearity is valid for this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part (c): Polynomial Regression\n",
    "\n",
    "We proceed to higher-order polynomial models for regression:\n",
    "\n",
    "- By visual inspection, what polynomial degree do you think would provide the best fit for the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- At the start of this assignment, we had advised you to normalize the time predictor in the training and test sets to a value in [0,1], and noted that this would be helpful in fitting polynomial regression models. Had the time predictor not been normalized, what difficulties in implementing polynomial regression may have occurred?\n",
    "\n",
    "- Fit polynomial regression models of degrees 2, 3, 10, 25 and 50 to the training set, and generate visualizations of the fitted models (in the same figure, plot the predicted value from all models as a function of time). \n",
    "\n",
    "- Evaluate the $R^2$ value of the fitted models on both the training and test sets. Does a high training $R^2$ value necessarily indicate a high test $R^2$ value? How do the test $R^2$ values from the different polynomial models compare with the test $R^2$ from simple linear regression in Part (b), and the best test $R^2$ from k-NN regression in Part (a)?\n",
    "\n",
    "- Generate residual plots for the different polynomial regression models (plot of residuals on training set vs. time). How does the increase in polynomial degree effect the residual plots?\n",
    "\n",
    "*Hint:* You may use the `PolynomialFeatures` class to include polynomial terms in the regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part (d): Summarize Results\n",
    "In a brief paragraph (8 or fewer sentences), summarize which of the models seen above you would choose to predict the number of taxi cab pick-ups at any specific time of day.  Be sure to explain your choice. Interpret the model you choose, including which predictors are significant and provide and interpret the CIs for their coefficients (if you choose a regression model). How well does you model predict the number of taxi cab pick-ups?  How would you improve this model even further?  Feel free to refer to visual(s) above or provide a new one to make your case.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APCOMP209a - Homework Question\n",
    "\n",
    "Read Sections 1 and 2 of this [paper](https://www.researchgate.net/profile/Roberto_Togneri/publication/45094554_Linear_Regression_for_Face_Recognition/links/09e4150d243bd8b987000000/Linear-Regression-for-Face-Recognition.pdf). \n",
    "\n",
    "Briefly, we have a number of cleaned images of people's faces. The model leverages the concept that \"patterns from a single-object class lie on a linear subspace\" and the fact that linear regression can be thought of as an orthogonal projection of the response vector (Y) onto the subspace spanned by the columns of the predictor matrix (X).\n",
    "\n",
    "### Question 1\n",
    "Consider a space in $\\mathbb{R}^5$, with two subspaces $S_0 \\subset \\mathbb{R}^5$ and $S_1 \\subset \\mathbb{R}^5$. For simplicity, let us consider the case where the subspace $S_0$ is spanned by the vectors $v_{00} = [1,0,0,0,0]^T$, $v_{01} =[0,1,0,0,0]^T$ and the subspace $S_1$ is spanned by the vectors $v_{10} = [0,0,0,1,0]^T$ and $v_{11} = [0,0,0,0,1]^T$. \n",
    "\n",
    "Now let us assume that we have a dataset that consists of labeled vectors in subspaces $S_0$ and $S_1$ respectively. Our task here is to use the data in the training dataset to classify an unknown vector into either $S_0$ or $S_1$.\n",
    "\n",
    "Let us consider the case where we construct a predictive matrix $\\mathbf{X}$ from the 'training' data for which we know the labels (note: is not meant to match $S_0$ and $S_1$ above).\n",
    "\n",
    "```\n",
    "dataset = np.array([\n",
    "            [1,0,0,0,0],\n",
    "            [1,1,0,0,0],\n",
    "            [0,0,0,1,1],\n",
    "            [0,0,0,1,0],\n",
    "            ])\n",
    "labels = np.array([0,0,1,1])\n",
    "```\n",
    "\n",
    "(Briefly notice that the training data probably doesn't consist of vectors that are orthonormal in spanning $S_0$ or $S_1$).\n",
    "\n",
    "And we have an unknown vector, for which we want clasify as either a noisy example of a vector in either $S_0$ or $S_1$: ```y_0 = np.array([2,10,1,0,0])```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# starter code\n",
    "dataset = np.array([\n",
    "            [1,0,0,0,0],\n",
    "            [1,1,0,0,0],\n",
    "            [0,0,0,1,1],\n",
    "            [0,0,0,1,0],\n",
    "            ])\n",
    "labels = np.array([0,0,1,1])\n",
    "y_0 = np.array([2,10,1,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1a\n",
    "Recall the solution for the Least Squares problem and the 'hat' matrix $\\mathbf{H}$. Consider the case when $Y$ lies in the same subspace as the columns of $\\mathbf{X}$. In this case, how accurate should the prediction of $\\hat{Y}$ be?\n",
    "\n",
    "#### Question 1b\n",
    "Given the projection of $Y$ onto the space spanned by the columns of the X matrix. We can analyse the significance of the different predictor vectors (i.e. the building blocks that are used to reconstrct the $\\hat{Y}$ vector). Discuss how this may help one when analysing a classification decision by the model.\n",
    "\n",
    "#### Question 1c\n",
    "As discussed in the linked paper, we have data that has vectors that are classified into one subspace or another (our training dataset). We can use the projection interpretation of linear regression to make a classification decision of a new (unseen) vector into either $S_0$ or $S_1$. In other words, construct X matrices from the known vectors, and project the unknown vector onto the subspaces spanned by the various X matrices using the \"hat\" interpretation of linear regression. To do classification, we can calculate the minimum euclidean distance ($L_2$ norm) between the original vector and the projection. Use this method to classify ```y_0``` as belonging to either $S_0$ or $S_1$.\n",
    "\n",
    "**Feel free to run this same example on the face data presented in the paper - you will be doing this for next week's homework regardless**\n",
    "\n",
    "#### Question 1d (unrelated to the parts above)\n",
    "\n",
    "Explicitly derive the relationship between the F-statistic (from the usual F test) and the $R^2$ measure in multiple linear regression (i.e. write the F-statistic of a multiple linear regression model in terms of the $R^2$ of the same model)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
