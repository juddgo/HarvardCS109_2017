{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 109A/STAT 121A/AC 209A/CSCI E-109A\n",
    "\n",
    "## Standard Section 3: Prediction using kNN and Regression Methods\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2017**<br/>\n",
    "**Section Leaders: Nathaniel Burbank, Albert Wu, Matthew Holman <br/>**\n",
    "**Instructors: Pavlos Protopapas, Kevin Rader, Rahul Dave, Margo Levine** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>**Download this notebook from the CS109 repo or here:**</center>\n",
    "<center>**http://bit.ly/109_S3**</center>\n",
    "\n",
    "For this section, our goal is to get you familiarized with k-Nearest Neighbors, Linear, and Polynomial Regression. These methods find powerful applications in all walks of life and are centered around prediction. \n",
    "\n",
    "\n",
    "Specifically, we will: \n",
    "    \n",
    "    1. Review some data selection basics\n",
    "    2. Load in the iris dataset which is split into a training and testing dataset\n",
    "    3. Do some basic exploratory analysis of the dataset and go through a scatterplot\n",
    "    4. Write out the algorithm for kNN WITHOUT using the sklearn package\n",
    "    5. Use the sklearn package to implement kNN and compare to the one we did by hand\n",
    "    6. Extend the sklearn package to linear and polynomial regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section we will be using the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 999)\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Note --  Requires sklearn version .18 or higher  \n",
    "from sklearn import metrics, datasets\n",
    "from collections import Counter\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.api import OLS\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "%matplotlib inline\n",
    "\n",
    "assert(sys.version_info.major==3),print(sys.version)\n",
    "# Python 3 or higher is required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review â€“ Python list comprehensions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Makes a list of all the even numbers from 0 to 20 \n",
    "# with a for loop\n",
    "l = []\n",
    "for i in range(20):\n",
    "    if i%2 ==0:\n",
    "        l.append(i)\n",
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Makes a list of even numbers from 0 to 20 using a python list comprehension "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting by assigned index vs integer position "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_url = 'https://raw.githubusercontent.com/nathanielburbank/CS109/master/data/states.csv'\n",
    "states = pd.read_csv(data_url,index_col=0)\n",
    "states.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select just the columns from 1930 to 1960 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "states[['1930','1940','1950','1960']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select just New York, New Jersey, and Connecticut "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tristate = ['New York','New Jersey','Connecticut']\n",
    "## Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select just New York, New Jersey, and Connecticut from 1930 to 1960 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the states between Nevada and New York, from 1930 to 1960"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select rows 10 to 15 *by positon*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select rows 10 to 15, and the last five columns (by position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the iris dataset and EDA:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iris dataset can be found within the **sklearn** package and contains measurement data for three types of Iris' (a kind of flower): 1) Setosa, 2) Versicolour, and 3) Virginica. For each type of Iris, we have recorded the sepal length, sepal width, petal length, and petal width in centimeters. (The sepal can be basically thought of as the outer-most petal of a flower).  These four measurements were done on 50 unique Setosa, 50 unique Versicolour, and 50 unique Virginia flowers for a total of 150 unique flower measurements. \n",
    "\n",
    "In the dataset below, we will let the **target** variable designate the flower type by letting **0** represent Setosa, **1** represent Versicolour, and **2** represent Virginica. Now, we will load in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load in the dataset, which is contained in the sklearn package\n",
    "# Inital version of dataset is in dict-like container object\n",
    "iris_bunch = datasets.load_iris()\n",
    "\n",
    "# np.c_ is the numpy concatenate function which combines the data array and target array.\n",
    "# The target array is our \"Y\" variable and the data array are the \"X\" variables. \n",
    "iris = pd.DataFrame(data= np.c_[iris_bunch['data'], iris_bunch ['target']],\n",
    "                     columns= iris_bunch['feature_names'] + ['target'])\n",
    "\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the **describe()** function to summarize our dataset. When using the **describe()** function, some care should be taken to interpret the values based on what the data represent. For example, the **count** row shows we have 150 observations. The **target** column doesn't have too much interpretive value as the other columns as we are letting **0**, **1**, and **2** be **categorical** variables indicating which flower type was measured.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the groupby function to look at mean stats aggregated by flower type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris.groupby('target').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, it is good practice to normalize data before proceeding. As such, we can create the following functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Takes in a dataset and normalizes it. \n",
    "def normalize(x):\n",
    "    num = x - np.min(x)\n",
    "    denom = np.max(x) - np.min(x)\n",
    "    return (num / denom)\n",
    "\n",
    "iris.iloc[:, 0:4] = normalize(iris.iloc[:, 0:4])\n",
    "\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the describe function **AFTER** normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the **pairplot()** function to create a scatterplot matrix of our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_cols = iris.columns[:4]\n",
    "sns.pairplot(data=iris,hue='target',vars=features_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split up the data into a training set and a test set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an idea of what the data looks like, we would like to create a way to predict flower type (Setosa, Versicolour, or Virginica) based on our 4 predictor variables. Whatever method we use, it would be nice to have a way to assess how accurate our model is. Therefore, we will be breaking up the data into a **training** and a **testing** set. The **training** set will be used to train the model, while the **testing** set will be used to gauge how well our model does in general. The **testing** set is a way for us to ensure our model doesn't overfit our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first create a function that will randomly split the data up into a 70-30 split, with 70% of the data going into the **testing** set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(data):\n",
    "    # Determine the number of observations we have in our entire data set:\n",
    "    # YOUR CODE GOES HERE\n",
    "    \n",
    "    # Create a list of integer indices ranging over our number of observations:\n",
    "    # YOUR CODE GOES HERE\n",
    "    \n",
    "    # Use numpy's random.shuffle() function to randomly shuffle over our index:\n",
    "    # YOUR CODE GOES HERE\n",
    "    \n",
    "    # Create a list for the first 70% of the shuffled indices and set to training: \n",
    "    # YOUR CODE GOES HERE\n",
    "    \n",
    "    # Create a list for the remaining 30% of the shuffled indices and set to testing:\n",
    "    # YOUR CODE GOES HERE\n",
    "    \n",
    "    # Use the list of training indices to find the corresponding data entries:\n",
    "    # YOUR CODE GOES HERE\n",
    "    \n",
    "    # Use the list of testing indices to find the corresponding data entries:\n",
    "    # YOUR CODE GOES HERE\n",
    "    \n",
    "    # Return two dataframes, one with the testing data and one with the training data:\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now run the function and see if it returns actually what we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "iris_train,iris_test  = split_data(iris)\n",
    "\n",
    "# Return the dimensions of our training dataframe after using the split_data function:\n",
    "# YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative approach using train_test_split from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(iris, test_size=0.3)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the kNN Algorithm by hand:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To really understand how the kNN algorithm works, it helps to go through the algorithm line by line in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def knn_algorithm(train, test, k):\n",
    "    \n",
    "    # Create any empty list to store our predictions in:\n",
    "    predictions = []\n",
    "    \n",
    "    predictor_cols = [col for col in train.columns if col != 'target']\n",
    "    \n",
    "    # Separate the response and predictor variables from training and test set:\n",
    "    train_x = train[predictor_cols]\n",
    "    train_y = train['target']\n",
    "    test_x  = test[predictor_cols]\n",
    "    test_y  = test['target']\n",
    "    \n",
    "    for index, row in test_x.iterrows():\n",
    "\n",
    "        # For each test point, store the distance between all training points and test point\n",
    "        vec_distances = pd.DataFrame((train_x.values - row.values)**2, index=train.index, columns = train_x.columns)\n",
    "\n",
    "        # Then, we sum across the columns per row to obtain the Euclidean distance squared\n",
    "        distances = vec_distances.sum(axis = 1)\n",
    "        \n",
    "        # Sort the distances to training points (in ascending order) and take first k points\n",
    "        nearest_k = distances.sort_values().iloc[:k]\n",
    "        # For simplicity, we omitted the square rooting of the Euclidean distance because the\n",
    "        # square root function preserves order. \n",
    "        \n",
    "        # Take the mean of the y-values of training set corresponding to the nearest k points\n",
    "        k_mean = train_y[nearest_k.index].mean()\n",
    "        \n",
    "        # Add on the mean to our predicted y-value list\n",
    "        predictions.append(k_mean)\n",
    "    \n",
    "    # Create a dataframe with the x-values from test and predicted y-values  \n",
    "    predict = test.copy()  \n",
    "    predict['target'] = pd.Series(predictions, index=test.index)\n",
    "    \n",
    "    return predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to run the algorithm on our dataset with $k = 5$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = 5\n",
    "predicted_knn = knn_algorithm(iris_train, iris_test, k)\n",
    "predicted_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to have a way to evaluate our predictions from the kNN algorithm with $k=5$. One way is to compute the $R^2$ coefficient. Let's create a function for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(predicted, true):\n",
    "    # Find the squared error:\n",
    "    squared_error = (predicted['target'] - true['target'])**2\n",
    "    \n",
    "    # Finding the mean squared error:\n",
    "    error_var = squared_error.sum()\n",
    "    sample_var = ((true['target'] - true['target'].mean())**2).sum()\n",
    "    \n",
    "    r = (1 - (error_var / sample_var))\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's apply this function to our predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluate(predicted_knn, iris_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the coefficient for the Nearest Neighbors implementation with $k=5$ is $R^2 = 0.9745$, which should more or less match what we get with the sklearn package. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now using sklearn to implement kNN:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use the **sklearn** package to implement kNN:\n",
    "\n",
    "Here, we will split our data using the train_test_split function from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we can use sklearn's train_test_split function to split our data:\n",
    "train, test =  train_test_split(iris, test_size=.3)\n",
    "\n",
    "x_train, x_test = train[features_cols], test[features_cols]\n",
    "y_train, y_test = train['target'], test['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can fit the model and use various metrics to assess our accuracy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also introduce a Confusion Matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set kNN parameter:\n",
    "k = 10\n",
    "\n",
    "# Now we can fit the model, predict our variable of interest, and then evaluate our fit:\n",
    "# First, we create the classifier object:\n",
    "neighbors = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "# Then, we fit the model using x_train as training data and y_train as target values:\n",
    "neighbors.fit(x_train, y_train)\n",
    "\n",
    "# Retreieve our predictions:\n",
    "prediction_knn = neighbors.predict(x_test)\n",
    "\n",
    "# This returns the mean accuracy on the given test data and labels, or in other words, \n",
    "# the R squared value -- A constant model that always predicts the expected value of y, \n",
    "# disregarding the input features, would get a R^2 score of 1.\n",
    "r = neighbors.score(x_test, y_test)\n",
    "r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expected_knn = y_test\n",
    "predicted_knn = neighbors.predict(x_test)\n",
    "print(metrics.classification_report(expected_knn, predicted_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(metrics.confusion_matrix(expected_knn, predicted_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, 4 flowers belonging to class 3 were mis-classified as class 2, while 2 flowers of class 2 were mis-classified as class 3. A confusion matrix allows us to view where our inaccurate predictions lie in a simple \"snapshot\" style matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear and Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just went over the kNN prediction method. Now, we will fit the same data, but onto linear and polynomial regressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the training/testing dataset as before and create our linear regression objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from statsmodels.api import OLS\n",
    "# We must first create the linear regression object from sklearn:\n",
    "regr = LinearRegression()\n",
    "# Then, we will put in the training sets in for the .fit() function:\n",
    "regr.fit(x_train, y_train)\n",
    "# This prints the regression coefficients of our model:\n",
    "print(regr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "# We must first create the linear regression object from stats model:\n",
    "model = sm.OLS(y_train.values, x_train)\n",
    "regr = model.fit()\n",
    "# This prints the regression coefficients of our model:\n",
    "regr.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will compute metrics that can be used to assess fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To compute the mean squared error (notice that we are now using the TEST set):\n",
    "np.mean((regr.predict(x_test)-y_test)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regr.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instead of focusing on $R^2$, letâ€™s look at the classification stats.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_knn = np.round(regr.predict(x_test))\n",
    "print(metrics.classification_report(expected_knn, predicted_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(metrics.confusion_matrix(expected_knn, predicted_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial regression is useful when you suspect a non-linear relationship between the predictor variables $x$ and the conditional expectation of $y$. Specifically, it is a special case of linear regression where the predictor variables are modeled through an $n$th degree polynomial. In Python, we can create the polynomial features through scikit-learn's PolynomialFeatures package. Then, we can use linear regression to implement a polynomial regression model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to create the PolynomialFeatures object and specify to what degree we wish to take our polynomial to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the PolynomialFeatures object and specify the number of degrees:\n",
    "degree = 2\n",
    "poly = PolynomialFeatures(degree)\n",
    "\n",
    "x_train_poly = poly.fit_transform(x_train)\n",
    "x_test_poly = poly.fit_transform(x_test)\n",
    "pd.DataFrame(x_train_poly).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a linear regression object\n",
    "lg = LinearRegression()\n",
    "\n",
    "# Fit our training data with polynomial features \n",
    "lg.fit(x_train_poly, y_train)\n",
    "\n",
    "# Obtain coefficients\n",
    "lg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can also do prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predict from our fitted polynomial regression model based on our test set. \n",
    "poly.predicted = lg.predict(x_test_poly)\n",
    "poly.predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Computer the Mean Squared Error for Polynomial Regression:\n",
    "np.mean((poly.predicted-y_test)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_knn = np.round(poly.predicted)\n",
    "print(metrics.classification_report(expected_knn, predicted_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(metrics.confusion_matrix(expected_knn, predicted_knn))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
