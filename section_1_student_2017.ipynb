{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 109A/STAT 121A/AC 209A/CSCI E-109A\n",
    "\n",
    "## Standard Section 1: Introduction to Webscraping with BeautifulSoup \n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2017**<br/>\n",
    "**Section Leaders: Albert Wu, Matthew Holman, Nathaniel Burbank <br/>**\n",
    "**Instructors: Pavlos Protopapas, Kevin Rader, Rahul Dave, Margo Levine** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes the most interesting data sets don’t come with an API or pre-packaged plain-text CSVs. In these situations, web scraping can be a powerful tool, enabling us to extract and convert data from almost any format found on the internet into a tabular form we can conduct further analysis on. \n",
    "\n",
    "For this section we will be working with historic population estimates of different US states listed on Wikipedia here:https://en.wikipedia.org/wiki/List_of_U.S._states_by_historical_population. In this section, you’ll learn how to import html-tables on the web into something you can manipulate with pandas. \n",
    "\n",
    "Specifically, we will: \n",
    "    \n",
    "    1. Download the relevant webpage as raw html with the requests library  \n",
    "    2. Parse the html into a tree-like python-object with the BeautifulSoup library\n",
    "    3. Use BeautifulSoup to select and extract just the tables we’re interested in\n",
    "    4. Combine the tables, clean the text, and convert them into a single python dictionary \n",
    "    5. Make a pandas dataframe from the dictionary \n",
    "    6. Doing some basic analysis and plotting (with matplotlib) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, our first step is to import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "\n",
    "# Two new \n",
    "import requests \n",
    "requests.packages.urllib3.disable_warnings()\n",
    "#Requests enables us to download raw html as text\n",
    "from bs4 import BeautifulSoup \n",
    "# BeautifulSoup enables us to navigate html in python with dom-like tree structure "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Download the relevant webpage as raw html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = \"http://en.wikipedia.org/wiki/List_of_U.S._states_by_historical_population\"\n",
    "r = requests.get(url, timeout=20,verify=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always remember to “not to be evil” when scraping with requests! If downloading multiple pages (like you will be on HW1), always put a delay between requests (e.g,, time.sleep(1), with the time library) so you don’t unwittingly hammer someone’s webserver and/or get blocked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_html = r.text\n",
    "raw_html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have all our data in the notebook. Unfortunately, it is the form of one really long string, which is hard to work with directly. This is where BeautifulSoup comes in.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Parse the html with BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(raw_html, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key BeautifulSoup functions we’ll be using in this section:\n",
    "- **soup.prettify()**: Returns cleanedup version of raw html for printing\n",
    "- **soup.find_all(<htmltag>,attrs=<attributes>)**: Returns python list of matching objects\n",
    "- **soup.find(<htmltag>,attrs=<attributes>)**: Returns first matching object \n",
    "- **soup.text/soup.get_text()**: Returns visible text of an object (e.g.,\"`<p>Some text</p>`\" -> \"Some text\")\n",
    "    \n",
    "BeautifulSoup is a very powerful library -- much more info here: https://www.crummy.com/software/BeautifulSoup/bs4/doc/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's practice some BeautifulSoup commands.. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print a cleanedup version of raw html for printing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your code here  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Find the first “title” object ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Find the text of first “title” object ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: extract just the tables we’re interested in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tables = soup.find_all(\"table\",attrs={\"class\":\"wikitable\"})\n",
    "# This says return a list of all table objects that include \n",
    "# the css class “wikitable” within the soup object.  \n",
    "len(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out, the data we’re interested in is in the 1st, 3rd, and 4th wikitable table on the page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tables = [tables[0], tables[2], tables[3]]\n",
    "len(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Convert the html tables into a python dictionary \n",
    "\n",
    "Before we can think about how to extract what we need, we need to understand how tables are constructed in HTML..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"\"\"<table>\n",
    "  <tr>\n",
    "    <th>Firstname</th>\n",
    "    <th>Lastname</th> \n",
    "    <th>Age</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Jill</td>\n",
    "    <td>Smith</td> \n",
    "    <td>50</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Eve</td>\n",
    "    <td>Jackson</td> \n",
    "    <td>94</td>\n",
    "  </tr>\n",
    "</table>\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s look at the first table in the list of tables we’re trying to import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's render the first table as HTML\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(tables[0].prettify()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to associate a series of population values with each state, so that we can build a time series table across all three tables. Things to keep in mind while building our extract routine:\n",
    "\n",
    "    - We don’t care about the “admitted” column \n",
    "    - We want to remove the commas from the numbers so python interprets them as ints rather than strings \n",
    "    - We want to remove the footnotes and links \n",
    "\n",
    "Now, let’s try to extract out our data…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rm_footnotes(s):\n",
    "    \"\"\"\n",
    "    Removes any text after first '[' in string\n",
    "    District of Columbia[1] -> District of Columbia\"\"\"\n",
    "    # Your code here \n",
    "\n",
    "def clean_int(s):\n",
    "    \"\"\"Removes any commas or footnotes from string and converts to int.\n",
    "       Returns zero for blank strings\"\"\"\n",
    "    # Your code here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = dict()\n",
    "\n",
    "for table in tables:\n",
    "    \"\"\"Extracts population data for states from all tables in tables \n",
    "      and store it in single dictionary d \"\"\"\n",
    "    \n",
    "    headrow = table.find(\"tr\")\n",
    "    \n",
    "    col_names = [(idx,th.text) for idx,th in enumerate(headrow.find_all('th')) if th.text.isnumeric()]\n",
    "    # Makes list of tuples like this with idx and name for cols with years.\n",
    "    # By using isnumeric, we only include columns are that are years. \n",
    "    # Result looks like this: \n",
    "    # [(2, '1790'), (3, '1800'), (4, '1810')]                     \n",
    "    \n",
    "    rows = table.find_all(\"tr\")[1:]\n",
    "    # List of rows in table excluding the header row \n",
    "    \n",
    "    for row in rows:\n",
    "        state_name = rm_footnotes(row.find('td').text)\n",
    "        # String of state name, with any footnotes removed \n",
    "        \n",
    "        all_cells = [c.text for c in row.find_all('td')]\n",
    "        # List of cell values for row, e.g.: \n",
    "        # ['Alabama', '1819', '\\xa0', '1,250', '9,046' .. ] \n",
    "        \n",
    "        existing_values = d.get(state_name,{})\n",
    "        # Existing dict of values for given state \n",
    "        \n",
    "        new_values = {year:clean_int(all_cells[idx]) for (idx,year) in col_names}\n",
    "        # For cols listed in col_names, return dict of cleaned int values \n",
    "        # {'1790': 0, '1800': 1250, '1810': 9046...}\n",
    "        \n",
    "        existing_values.update(new_values)\n",
    "        # Merge with existing dict for state \n",
    "        d[state_name] = existing_values\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Make a pandas dataframe from the dictionary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all three tables in a cleaned dictionary form, with each state as a key, and time series for each sate (as another dict) as it’s value, it's simple to convert to a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(d,orient='index')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks pretty good! However, we’ve accidently imported the total row (United States). We could address that in our function above, or just drop it from our dataframe.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.drop([\"United States\"])  \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An alternative aproach -- Using pd.read_html() instead\n",
    "\n",
    "In this particular situation, using Panda’s built-in “read_html” function actually works pretty well, and would have saved us a lot of code above. However, not all datasets come in nicely formatted html tables, so it’s important to develop the skills to write a scraping routine from scratch from any type of html page (as we did above)\n",
    "\n",
    "More info here: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_html.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.read_html(tables[0].prettify(),header=0, index_col=0)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note – if we really wanted to take this approach, we’d have to handle removing the footnotes and merging/joining the tables in Pandas.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: let's plot our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.T.plot(legend=None)\n",
    "plt.ylabel(\"Population (Millions)\")\n",
    "plt.xlabel('Year \\n\\n (Every line represents the population for an individual state overtime)')\n",
    "plt.title(\"US Population by State, 1790-2010\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's look at just the 10 largest states (2010 population)\n",
    "df.sort_values('2010', ascending=False)\n",
    "largest = df['2010'].nlargest(10).index\n",
    "df[df.index.isin(largest)].T.plot()\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "plt.title(\"Population by State for 10 largest states, 1790-2010\")\n",
    "plt.ylabel(\"Population (Millions)\")\n",
    "plt.xlabel('Year \\n\\n (Every line represents the population for an individual state overtime)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps it will be more interested to view as percentage of total US population, so we can see relative change overtime…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick\n",
    "cols = df.columns\n",
    "by_percent = df[cols].div(df[cols].sum(axis=0), axis=1).multiply(100)\n",
    "by_percent[by_percent.index.isin(largest)].T.plot()\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "plt.ylabel(\"Percent of total US population\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.title(\"Percentage of total US Population by State, 1790-2010\")\n",
    "\n",
    "# Make y-axis percentages \n",
    "ax = plt.gca()\n",
    "fmt = '%.0f%%' # Format you want the ticks, e.g. '40%'\n",
    "yticks = mtick.FormatStrFormatter(fmt)\n",
    "ax.yaxis.set_major_formatter(yticks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "by_percent[by_percent.index.isin(largest)].T.plot(kind=\"bar\",stacked=True)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "plt.ylabel(\"Percent of total population\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.title(\"Percentage of total US Population by State, 1790-2010\")\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.yaxis.set_major_formatter(yticks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
