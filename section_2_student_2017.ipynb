{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 109A/STAT 121A/AC 209A/CSCI E-109A\n",
    "\n",
    "## Standard Section 2: Webscraping with BeautifulSoup, Continued\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2017**<br/>\n",
    "**Section Leaders: Nathaniel Burbank, Albert Wu, Matthew Holman <br/>**\n",
    "**Instructors: Pavlos Protopapas, Kevin Rader, Rahul Dave, Margo Levine** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>**Download this notebook from CS109 repo or here: <br> http://bit.ly/109_s2 **</center>\n",
    "\n",
    "\n",
    "During the first section we worked with tables of historic population estimates of the different US states, as listed on the Wikipedia site: https://en.wikipedia.org/wiki/List_of_U.S._states_by_historical_population.  We learned how to download a webpage via its URL, to parse the resulting html into a python object with BeautifulSoup, to extract the necessary tables, to convert the results into a pandas dataframe, and finally to make a few plots.\n",
    "\n",
    "In this section, we will extend those results to include the median household income and other information per state. \n",
    "Specifically, we will: \n",
    "    \n",
    "    1. Review the process of downloading a webpage with the requests library \n",
    "    2. Revisit how to parse the html into a python object using the BeautifulSoup library\n",
    "    3. Again use BeautifulSoup to select and extract the relevant tables\n",
    "    4. Clean the text and convert the table into a python dictionary \n",
    "    5. Save and restore the results in files with JSON and csv (pandas)\n",
    "    6. Make a pandas dataframe from the dictionary\n",
    "    7. Join the new dataframe with the one we made last week.\n",
    "    8. Inner vs outer joins \n",
    "    9. Do more analysis and plotting (with matplotlib) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, our first step is to import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "\n",
    "import requests \n",
    "requests.packages.urllib3.disable_warnings()\n",
    "#Requests enables us to download raw html as text\n",
    "from bs4 import BeautifulSoup \n",
    "# BeautifulSoup enables us to navigate html in python with dom-like tree structure \n",
    "\n",
    "# Two new ones\n",
    "import time\n",
    "# So that we can sleep between downloads\n",
    "import json\n",
    "# We'll write and read JSON files later\n",
    "\n",
    "assert(sys.version_info.major==3),print(sys.version)\n",
    "# Python 3 or higher is required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download, extract, and parse the relevant webpage as raw html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's quickly reconstruct the pandas dataframe we made in the last section.  Recall that we\n",
    "\n",
    "    1. used the requests module to download the page corresponding to a given URL,\n",
    "    2. extracted the raw HTML of the result, and\n",
    "    3. used BeautifulSoup to parse the raw HTML.\n",
    "    \n",
    "We could repeat each of these steps, but you might have noticed that we only used the final result and ignored the intermediate results.  This calls out for a function definition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dictionary to store already downloaded BeautifulSoup objects \n",
    "html_cache = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dl_parsed_html(url, timeout=20, verify=False, sleep_time=1):\n",
    "    \"\"\"Downloads the content of the page specified by url.\n",
    "    Return the parsed HTML or None upon error.\n",
    "    Notice that timeout, verify, and sleep_time have \n",
    "    defaults values\"\"\"\n",
    "    \n",
    "    # Down load the context at url\n",
    "    r = requests.get(url, timeout=20,verify=False)\n",
    "    \n",
    "    # Check if HTTP status code is anything other than \"ok\"\n",
    "    if r.status_code != 200:\n",
    "        return None\n",
    "    \n",
    "    # Grab the raw HTML of the result\n",
    "    raw_html = r.text\n",
    "    \n",
    "    # Parse the raw HTML with Beautiful soup\n",
    "    soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "    \n",
    "    # We have added a default 1 sec sleep\n",
    "    time.sleep(sleep_time)\n",
    "    \n",
    "    # Return the parsed results\n",
    "    return soup\n",
    "\n",
    "def get_parsed_html(url, timeout=20, verify=False, sleep_time=1):\n",
    "    \"\"\"If we’ve already downloaded a webpage, return from cache. \n",
    "    Otherwise, download and parse using dl_parsed_html above.\"\"\"\n",
    "    if url not in html_cache:\n",
    "        parsed_html = dl_parsed_html(url, timeout, verify, sleep_time)\n",
    "        if parsed_html:\n",
    "            html_cache[url] = parsed_html \n",
    "            \n",
    "    return html_cache.get(url,None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use our new function to download and parse the US historical population information again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soup = get_parsed_html(\"http://en.wikipedia.org/wiki/List_of_U.S._states_by_historical_population\")\n",
    "soup.find('title').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use find_all to get the tables of interest again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tables = soup.find_all(\"table\",attrs={\"class\":\"wikitable\"})\n",
    "# This says return a list of all table objects that include \n",
    "# the css class “wikitable” within the soup object.  \n",
    "\n",
    "# We only need the first, third, and fourth tables.\n",
    "tables = [tables[0], tables[2], tables[3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can render the first table, to inspect it, as we did in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(tables[0].prettify()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data and build dictionary\n",
    "\n",
    "Here is the code we used in the previous section to clean the data and build the dictionary, row by row, from a list of tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rm_footnotes(s):\n",
    "    \"\"\"Removes any text after first '[' in string\n",
    "    District of Columbia[1] -> District of Columbia\"\"\"\n",
    "    return s.split('[')[0]\n",
    "\n",
    "def clean_int(s):\n",
    "    \"\"\"Removes any commas, '$' symbols, or footnotes from string and converts to int.\n",
    "    Returns zero for blank strings\"\"\"\n",
    "    s = s.strip().replace(',','').replace('$', '')\n",
    "    s = rm_footnotes(s)\n",
    "    return int(s) if s else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = dict()\n",
    "\n",
    "for table in tables:\n",
    "    \"\"\"Extracts population data for states from all tables in tables \n",
    "      and store it in single dictionary d \"\"\"\n",
    "    \n",
    "    headrow = table.find(\"tr\")\n",
    "    \n",
    "    col_names = [(idx,th.text) for idx,th in enumerate(headrow.find_all('th')) if th.text.isnumeric()]\n",
    "    # Makes list of tuples like this with idx and name for cols with years.\n",
    "    # By using isnumeric, we only include columns are that are years. \n",
    "    # Result looks like this: \n",
    "    # [(2, '1790'), (3, '1800'), (4, '1810')]                     \n",
    "    \n",
    "    rows = table.find_all(\"tr\")[1:]\n",
    "    # List of rows in table excluding the header row \n",
    "    \n",
    "    for row in rows:\n",
    "        state_name = rm_footnotes(row.find('td').text)\n",
    "        # String of state name, with any footnotes removed \n",
    "        \n",
    "        all_cells = [c.text for c in row.find_all('td')]\n",
    "        # List of cell values for row, e.g.: \n",
    "        # ['Alabama', '1819', '\\xa0', '1,250', '9,046' .. ] \n",
    "        \n",
    "        existing_values = d.get(state_name,{})\n",
    "        # Existing dict of values for given state \n",
    "        \n",
    "        new_values = {year:clean_int(all_cells[idx]) for (idx,year) in col_names}\n",
    "        # For cols listed in col_names, return dict of cleaned int values \n",
    "        # {'1790': 0, '1800': 1250, '1810': 9046...}\n",
    "        \n",
    "        existing_values.update(new_values)\n",
    "        # Merge with existing dict for state \n",
    "        d[state_name] = existing_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use JSON to save and restore data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON (JavaScript Object Notation) is a simple data exchange format that can be used to serialize many data objects. To serialize an object means to convert it to a format that can be transmitted in a series, either to or from files or across a network.  This is particularly easy in python.  (See [pickle](https://docs.python.org/3/library/pickle.html) for an alternative means of 'serializing' python objects that supports a larger range of python objects but is not human readable (and not a universal format like json)\n",
    "\n",
    "Here we'll use JSON to save our dictionary so that we can just load it from the file, if we need in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = \"historical_population.json\"\n",
    "\n",
    "# Open a file descriptor for writing, with the filename 'historical_population.json'.\n",
    "# Convert our dictionary 'd' to JSON and output it to the file.\n",
    "# Close the file.\n",
    "\n",
    "# Your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convenient feature of JSON is that the resulting files are ASCII and thus simple to read (although they are larger than a corresponding binary format would be).\n",
    "\n",
    "Let's use a shell command to look at the actual file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cat historical_population.json\n",
    "# exclamation point here is a \"magic\" command that means \n",
    "# run line as shell command in terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks a lot like a python dictionary!\n",
    "\n",
    "Let's demonstrate the use of JSON files by deleting d and then reloading it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now open the json file back up and save the results to d\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We successfully restored the dictionary from its JSON file!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the dataframe\n",
    "\n",
    "Now that we have the dictionary, it's straightfoward to generate a pandas dataframe from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate the dataframe from the dictionary.\n",
    "df = pd.DataFrame.from_dict(d,orient='index')\n",
    "\n",
    "# Drop unneeded rows.\n",
    "df = df.drop([\"United States\"])  \n",
    "\n",
    "# Sort the columns in increasing (alphabetical) order.\n",
    "df = df.reindex_axis(sorted(df.columns), axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative to saving JSON – save dataframe object as csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"historical_population.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cat historical_population.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.from_csv(\"historical_population.csv\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting as csv is another useful method for saving a data frame offline you can easily jump back to your dataset without needed to re-scrape from Wikipedia. Note however that when we export as csv, no metadata about the data frame is saved which can cause subtle differences on re-import in the datatypes of your columns and which column is used as the index.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download, extract, and parse another webpage\n",
    "\n",
    "Now that we have regenerated the dictionary and dataframe from last section, let's demonstrate a slightly different approach to downloading, extracting, and parsing.   This time we will look at a webpage of the US states by income.\n",
    "\n",
    "We'll re-use our previous function to download and parse the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soup = get_parsed_html(\"https://en.wikipedia.org/wiki/List_of_U.S._states_by_income\")\n",
    "soup.find('title').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tables = soup.find_all(\"table\",attrs={\"class\":\"wikitable\"})\n",
    "# This says return a list of all table objects that include \n",
    "# the css class “wikitable” within the soup object.  \n",
    "len(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This page has two wikitables, and we are interested in the second one, which looks like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's render the second table as HTML\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(tables[1].prettify()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the html table directly into a pandas dataframe\n",
    "\n",
    "For well formatted HTML tables the pandas read_html method works like a charm, and it can be much easier than building a dataframe row by row. \n",
    "\n",
    "With the read_html method, we can specify which row has the column names and which column is the index.  We'll start by using the first row and first column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_html(tables[1].prettify(),header=0, index_col=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few things we should clean up before proceeding. First let's set the index to be the'State' column.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = df1.set_index('State')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that by overwriting the index, we’re discarding the 'Rank' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's drop the 'United States' row -- but keep your eye on the District of Columbia and Puerto Rico rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = df1.drop([\"United States\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, the column names have one or more spaces embedded in them.  That can make the columns difficult to select.  Fortunately, a pandas dataframe allows easy access to the column names as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this is a numpy array of type 'object' with a list of column names.\n",
    "We can write a quick reformatting function and apply it to all the column names.  \n",
    "\n",
    "Here we use both 'strip' which is a built_in method of string objects and the 're' (regular expression) substitution method.  Regular expressions are a powerful, concise way of representing an enormous range of string transformations.  \n",
    "\n",
    "To learn more about regular expresions in python, see:\n",
    "https://docs.python.org/3/library/re.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_column_name(s):\n",
    "    \"\"\"This removes any leading or trailing spaces and\n",
    "    replaces multiple embedded spaces with a single\n",
    "    underscore character\n",
    "    \n",
    "    'Per capita  income' -> 'Per_capita_income'\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    \n",
    "    return s\n",
    "\n",
    "new_column_names = list(map(clean_column_name, list(df1.columns.values)))\n",
    "\n",
    "# Here's our set of reformatted column names\n",
    "new_column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can reset the column names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1.columns = new_column_names\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better!\n",
    "\n",
    "Let’s say we were interested in identifying states that had a large divergence between Median household income and Median family income. \n",
    "\n",
    "Per the Census Bureau’s [definitions](https://www.census.gov/programs-surveys/cps/technical-documentation/subject-definitions.html#familyhousehold), a family is “a group of two people or more (one of whom is the householder) related by birth, marriage, or adoption and residing together; all such people (including related subfamily members) are considered as members of one family.” Conversely, a household “consists of all the people who occupy a housing unit.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1.Median_family_income - df1.Median_household_income"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh oh --- that did not work – what’s wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not work becuase these two columens are represented by strings rather than ints or float objects. On import, pandas interpreted them as strings rather than floats or ints because of the '\\$' symbols in them. Let's use our clean_int function from last time to solve this issue.  (Notice, that clean_int now replaces '\\$' symbols too.)  If we 'apply' clean_int to the Per_capita_income column, we get the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1['Per_capita_income'].apply(clean_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have three such columns to deal with, let's clean them in bulk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for colname in ['Per_capita_income', 'Median_household_income', 'Median_family_income']:\n",
    "    df1[colname] = df1[colname].apply(clean_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does this work?  Aren't we changing the columns as we go?\n",
    "\n",
    "Now let’s get back to looking at the difference between median household and family by state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1['Household_family_diff'] = df1.Median_family_income - df1.Median_household_income\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s identify the states with the largest differential... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "key_cols = ['Median_family_income','Median_household_income','Household_family_diff']\n",
    "df1.sort_values(['Household_family_diff'],ascending=False)[key_cols].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the smallest... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1.sort_values(['Household_family_diff'])[key_cols].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_values = (df1['Household_family_diff']).sort_values(ascending=False)\n",
    "x_values = list(y_values.index)\n",
    "x_pos = np.arange(len(y_values))\n",
    "\n",
    "plt.bar(x_pos,y_values, align='center', alpha=0.5)\n",
    "plt.xticks(x_pos, x_values, rotation=90, fontsize=12)\n",
    "plt.ylabel('Difference between family income and household income (USD)')\n",
    "plt.title('Difference between Median family income and median household income by state')\n",
    "\n",
    "new_style = {'grid': False}\n",
    "plt.rc('axes', **new_style)\n",
    "plt.rc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make almost the exact same chart using Panda’s  built in “plot” command with much less code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1['Household_family_diff'].sort_values(ascending=False).plot(kind=\"bar\", alpha=.5, fontsize=12)\n",
    "plt.ylabel('Difference between family income and household income (USD)')\n",
    "plt.title('Difference between Median family income and median household income by state')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to combine out two dataframes?   There are a number of ways to do this.  \n",
    "\n",
    "One way is to use the Pandas concat function.  It stitches the two data frames together based on their **index values**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = pd.concat([df, df1], axis=1, join='inner')\n",
    "print(len(result))\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the results by comparing the 'Connecticut' from each of the constituent dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.loc['Connecticut']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1.loc['Connecticut']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we can also do an 'outer' join.  What's the difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = pd.concat([df, df1], axis=1, join='outer')\n",
    "print(len(result))\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that “District of Columbia” is listed twice above. What’s going on? Why didn’t it match?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.index[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.index[8] == df1.index[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note even though the strings *look* the same, slight differences in the way the strings are encoded keep pandas from matching them up. If we wanted to fix this, we’d probably want to go back to the import stage and do it there. Adjusting indexes once they’ve been assigned is messy in pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = pd.concat([df, df1], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach is to use the merge method on the shared values in two specified columns.  Let's explore this by copying the index column to a column of a slightly different name.  (If we had available some other shared key, we could use that.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['state'] = df.index\n",
    "df1['state'] = df1.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_df= df1.merge(df, on='state', how='inner')\n",
    "len(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that the while the resulting table is correct, we’ve abounded our index which makes the table harder to work with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis on joined frame – what drives household income? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let’s explore a couple of possible drivers of median household income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(combined_df['2010']/1e6, combined_df['Median_household_income']/1e3)\n",
    "\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "plt.title(\"Median income vs population by State\")\n",
    "plt.xlabel(\"Population (Millions)\")\n",
    "plt.ylabel(\"Median household income (Thousands)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps we think population *growth*, rather than raw size is what matters…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "population_growth = result['2010']/result['1980']\n",
    "population_growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.title(\"Population growth by state, 1980-2010\")\n",
    "population_growth.sort_values(ascending=False).plot(kind='bar')\n",
    "plt.axhline(y=1, color='r', linestyle='-')\n",
    "# Draw a dotted line at y=1\n",
    "plt.ylabel(\"30 year population growth multiplier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(population_growth, combined_df.Median_household_income/1e3)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "plt.title(\"Median income vs population growth by State\")\n",
    "plt.xlabel(\"30 year population growth multiplier\")\n",
    "plt.ylabel(\"Median household income (Thousands)\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
